# lib pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torch.utils.data import Dataset, DataLoader

# lib torch_geometric for handling graph data
try:
    from torch_geometric.data import Batch
except ImportError:
    # Fallback for older versions
    try:
        from torch_geometric.loader import Batch
    except ImportError:
        # If torch_geometric is not available, create a dummy Batch
        class Batch:
            @staticmethod
            def from_data_list(data_list):
                # Simple fallback - just return the first item
                return data_list[0] if data_list else None

# lib default python
import os
import random
import time
import gc

# lib science math
import numpy as np

# lib sklearn
from sklearn.model_selection import StratifiedKFold, KFold

# my class
import config
import utils
import data
import train
from model import CNN_LSTM, CNN_BiLSTM, CNN_LSTM_METRICS, calculate_size_lstm
import time
import datetime
from config import argument
# my tool
from utils import write_file
import json

args = argument()

def custom_collate_fn(batch):
    """
    Custom collate function to handle torch_geometric Data objects and mixed data types
    Ensures all tensors are on the same device
    """
    if not batch:
        return None
    
    # Import config to get device
    import config
    device = config.Config.DEVICE
    
    # Check the structure of the first item to understand the batch format
    first_item = batch[0]
    
    if len(first_item) == 2:  # (data, label)
        data_list = []
        label_list = []
        
        for item in batch:
            data, label = item
            # Check if data contains graph data
            if hasattr(data, 'x') and hasattr(data, 'edge_index'):
                # This is graph data, move to device
                data = data.to(device)
                data_list.append(data)
            else:
                # Regular tensor data, move to device
                data = data.to(device)
                data_list.append(data)
            
            # Move label to device
            label = label.to(device)
            label_list.append(label)
        
        # Handle batching based on data type
        if hasattr(data_list[0], 'x') and hasattr(data_list[0], 'edge_index'):
            # Graph data
            batched_data = Batch.from_data_list(data_list)
        else:
            # Regular tensor data
            batched_data = torch.stack(data_list)
        
        batched_labels = torch.stack(label_list)
        return batched_data, batched_labels
        
    elif len(first_item) == 4:  # (data, metrics, label, graph)
        data_list = []
        metrics_list = []
        label_list = []
        graph_list = []
        
        for item in batch:
            data, metrics, label, graph = item
            
            # Move all tensors to device
            data = data.to(device)
            metrics = metrics.to(device)
            label = label.to(device)
            graph = graph.to(device)
            
            data_list.append(data)
            metrics_list.append(metrics)
            label_list.append(label)
            graph_list.append(graph)
        
        batched_data = torch.stack(data_list)
        batched_metrics = torch.stack(metrics_list)
        batched_labels = torch.stack(label_list)
        batched_graphs = Batch.from_data_list(graph_list)
        
        return batched_data, batched_metrics, batched_labels, batched_graphs
    
    else:
        # Fallback to default collate for other cases
        try:
            result = torch.utils.data.dataloader.default_collate(batch)
            # Move result to device if it's a tensor
            if isinstance(result, torch.Tensor):
                result = result.to(device)
            elif isinstance(result, (list, tuple)):
                result = tuple(item.to(device) if isinstance(item, torch.Tensor) else item for item in result)
            return result
        except:
            # If default collate fails, return batch as is
            return batch

if __name__ == "__main__":
    pos_weight_set = [
        torch.tensor(1.0, dtype=torch.float), 
        torch.tensor(2.0, dtype=torch.float), 
        torch.tensor(4.0, dtype=torch.float),
        torch.tensor(8.0, dtype=torch.float), 
        torch.tensor(12.0, dtype=torch.float), 
        torch.tensor(32.0, dtype=torch.float),
        torch.tensor(84.0, dtype=torch.float)
    ]

    kernel_size_set = [3, 4, 5, 6, 7]
    now = datetime.datetime.now()

    data_path = args.data_path
    graph_path = args.graph_path
    vocab_path = args.vocab_path
    with open(vocab_path, 'r') as f:
        vocabdict = json.load(f)
    vocab_size = len(vocabdict)

    result_summary = {}

    for pos_weight in pos_weight_set:
        for kernel_size in kernel_size_set:
            smell, model = utils.get_smell_and_model(args.data_path)   
            
            file_name = f'{model}_{smell}_{now.strftime("%d%m%Y_%H%M")}_posweight_{pos_weight.item()}_kernel_{kernel_size}'
            track_file = f'{args.tracking_dir}/{file_name}.txt'
            result_file = f'{args.result_dir}/{model}_{smell}_{now.strftime("%d%m%Y_%H%M")}.txt'
            
            precision = []
            recall = []
            f1 = []
            auc = []
            mcc = []
            
            if args.model == 'EnseSmells':
                load_dataset = enumerate(utils.get_CombineData_pickle(data_path))
            elif args.model == 'DeepSmells_TokenIndexing':
                load_dataset = enumerate(utils.get_data_token_indexing(data_path))
            elif args.model == 'EnseSmell_TokenIndexing' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                load_dataset = enumerate(utils.get_data_token_indexing_COMBINING(data_path, graph_path))
            else: # DeepSmells
                load_dataset = enumerate(utils.get_data_pickle(data_path))

            for index, datasets in load_dataset:
                print(f"{'=+'*25} FOLD: {index+1} / 5 {'+='*25}")
                write_file(track_file, f"{'=+'*25} FOLD: {index+1} / 5 {'+='*25}\n")
                # Sample elements randomly from given list of ids, no replacement
                if args.model == 'DeepSmells_METRICS' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                    train_set = data.DatasetCombineWithGraph(datasets.train_data, datasets.train_data_metrics, datasets.train_labels, datasets.train_graph_data)
                    valid_set = data.DatasetCombineWithGraph(datasets.eval_data, datasets.eval_data_metrics, datasets.eval_labels, datasets.eval_graph_data)
                if args.model == 'DeepSmells' or args.model == 'DeepSmells_TokenIndexing': 
                    train_set = data.Dataset(datasets.train_data, datasets.train_labels)
                    valid_set = data.Dataset(datasets.eval_data, datasets.eval_labels)

                # Define data loaders for training and testing data in this fold
                # Use custom collate function only if we detect graph data
                if args.model in ['DeepSmells_METRICS', 'DeepSmells_TokenIndexing_METRICS']:
                    # These models use graph data, so use custom collate function
                    train_loader = DataLoader(train_set, batch_size=args.train_batchsize, shuffle=True, collate_fn=custom_collate_fn)
                    valid_loader = DataLoader(valid_set, batch_size=args.valid_batchsize, shuffle=True, collate_fn=custom_collate_fn)
                else:
                    # Regular models without graph data
                    train_loader = DataLoader(train_set, batch_size=args.train_batchsize, shuffle=True)
                    valid_loader = DataLoader(valid_set, batch_size=args.valid_batchsize, shuffle=True)

                length_code = train_set[0][0].size()[1]
                # Calculate size LSTM - CC
                input_size_lstm = calculate_size_lstm(input_size=length_code, kernel_size=kernel_size)

                # Initialize the model, optimizer, scheduler, loss
                if args.model == 'DeepSmells' or args.model == 'DeepSmells_TokenIndexing':
                    model = CNN_LSTM(kernel_size = kernel_size, input_size_lstm=input_size_lstm, hidden_size_lstm=args.hidden_size_lstm).to(config.Config.DEVICE)
                if args.model == 'DeepSmells-BiLSTM':
                    model = CNN_BiLSTM(kernel_size = kernel_size, input_size_lstm=input_size_lstm, hidden_size_lstm=args.hidden_size_lstm).to(config.Config.DEVICE)
                if args.model == 'DeepSmells_METRICS' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                    metrics_size = train_set[0][1].size()[0]
                    model = CNN_LSTM_METRICS(kernel_size = kernel_size, vocab_size=vocab_size, embedding_dim=128,input_size_lstm=input_size_lstm, hidden_size_lstm=args.hidden_size_lstm, metrics_size = metrics_size).to(config.Config.DEVICE)
                optimizer = optim.SGD(model.parameters(), lr=args.lr)
                # step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)

                train_loss_fn, valid_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight), nn.BCEWithLogitsLoss(pos_weight=pos_weight)
                
                trainer = train.Trainer(
                    device = config.Config.DEVICE,
                    dataloader = (train_loader, valid_loader),
                    model = model,
                    loss_fns = (train_loss_fn, valid_loss_fn),
                    optimizer = optimizer,
                    # scheduler = step_lr_scheduler,
                )

                best_precision, best_recall, best_f1, best_auc, best_mcc = trainer.fit(
                    epochs = args.nb_epochs,
                    checkpoint_dir = None,
                    custom_name = file_name,
                    track_dir = track_file,
                    threshold = args.threshold,
                )

                precision.append(best_precision)
                recall.append(best_recall)
                f1.append(best_f1)
                auc.append(best_auc)
                mcc.append(best_mcc)
                # del model, optimizer, train_loss_fn, valid_loss_fn, trainer, best_precision, best_recall, best_f1, best_mcc
                gc.collect()
            
            result_summary[f"pos_weight_{pos_weight.item()}_kernel_{kernel_size}"] = [sum(precision)/len(precision), 
                                                                                  sum(recall)/len(recall), 
                                                                                  sum(f1)/len(f1), 
                                                                                  sum(auc)/len(auc), 
                                                                                  sum(mcc)/len(mcc)                                                                                                                                        
                                                                                ]
            write_file(result_file, f"pos_weight_{pos_weight.item()}_kernel_{kernel_size},{sum(precision)/len(precision)},{sum(recall)/len(recall)},{sum(f1)/len(f1)},{sum(auc)/len(auc)},{sum(mcc)/len(mcc)}\n")
    max_key = None
    max_f1 = 0
    for key, value in result_summary.items():
        # write_file(result_file, f"{key},{value[0]},{value[1]},{value[2]},{value[3]},{value[4]}\n")
        if value[2] > max_f1:
            max_f1 = value[2]
            max_key = key

    write_file(result_file, f"BEST-{max_key},{result_summary[max_key][0]}, {result_summary[max_key][1]}, {result_summary[max_key][2]}, {result_summary[max_key][3]}, {result_summary[max_key][4]}\n")








train.py..................................
from tqdm import tqdm
import config
import gc
import os

# sklearn lib
from sklearn import metrics
from sklearn.metrics import confusion_matrix

# pytorch lib
import torch
from torch.cuda.amp import GradScaler, autocast

# my tool
from utils import write_file
from config import Config, argument

args = argument()

class Trainer():
    def __init__(self, device, dataloader, model, loss_fns, optimizer, scheduler=None):
        self.device = device
        self.train_loader, self.valid_loader = dataloader
        self.model = model
        self.train_loss_fn, self.valid_loss_fn = loss_fns
        self.optimizer = optimizer
        self.scheduler = scheduler
    
    def train_one_epoch(self):
        '''
            Train the model for 1 epochs
        '''
        self.model.train()
        train_pbar = tqdm(enumerate(self.train_loader), total=(len(self.train_loader)))
        train_preds, train_targets = [], []
        
        for idx, cache in train_pbar:
            if args.model == 'DeepSmells_METRICS' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                inputs = self._convert_if_not_tensor(cache[0], dtype=torch.float)
                inputs_metrics = self._convert_if_not_tensor(cache[1], dtype=torch.float)
                targets = self._convert_if_not_tensor(cache[2], dtype=torch.float)
                # Check if graph data is available (cache[3] would be graph data)
                if len(cache) > 3 and cache[3] is not None:
                    graph_data = cache[3]  # Graph data from DataLoader
                else:
                    graph_data = None
            else: 
                inputs = self._convert_if_not_tensor(cache[0], dtype=torch.float)
                targets = self._convert_if_not_tensor(cache[1], dtype=torch.float)
                graph_data = None  # No graph data for simple models

            with autocast(enabled=True):
                if args.model == 'DeepSmells_METRICS' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                    if graph_data is not None:
                        # If your model supports graph data, pass it here
                        outputs = self.model(inputs, inputs_metrics, graph_data)
                    else:
                        outputs = self.model(inputs, inputs_metrics)
                else:
                    outputs = self.model(inputs)

                loss = self.train_loss_fn(outputs, targets)
                loss_itm = loss.item()

                train_pbar.set_description(f'train_loss: {loss_itm:.4f}')

                config.Config.SCALER.scale(loss).backward()
                config.Config.SCALER.step(self.optimizer)
                config.Config.SCALER.update()
                self.optimizer.zero_grad()
                if self.scheduler is not None:
                    self.scheduler.step()

            train_preds.extend(outputs.cpu().detach().numpy().tolist())
            train_targets.extend(targets.cpu().detach().numpy().tolist())
        
        del outputs, targets, inputs, loss_itm, loss
        gc.collect()

        return train_preds, train_targets

    @torch.no_grad()
    def valid_one_epoch(self):
        '''
            Validates the model for 1 epoch
        '''
        self.model.eval()
        valid_pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader))
        valid_preds, valid_targets = [], []

        for idx, cache in valid_pbar:
            if args.model == 'DeepSmells_METRICS' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                inputs = self._convert_if_not_tensor(cache[0], dtype=torch.float)
                inputs_metrics = self._convert_if_not_tensor(cache[1], dtype=torch.float)
                targets = self._convert_if_not_tensor(cache[2], dtype=torch.float)
                # Check if graph data is available
                if len(cache) > 3 and cache[3] is not None:
                    graph_data = cache[3]
                else:
                    graph_data = None
            else: 
                inputs = self._convert_if_not_tensor(cache[0], dtype=torch.float)
                targets = self._convert_if_not_tensor(cache[1], dtype=torch.float)
                graph_data = None

            if args.model == 'DeepSmells_METRICS' or args.model == 'DeepSmells_TokenIndexing_METRICS':
                if graph_data is not None:
                    outputs = self.model(inputs, inputs_metrics, graph_data)
                else:
                    outputs = self.model(inputs, inputs_metrics)
            else:
                outputs = self.model(inputs)

            valid_loss = self.valid_loss_fn(outputs, targets)
            valid_pbar.set_description(desc=f"valid_loss: {valid_loss:.4f}")

            valid_preds.extend(outputs.cpu().detach().numpy().tolist())
            valid_targets.extend(targets.cpu().detach().numpy().tolist())
        
        del outputs, targets, inputs, valid_loss
        gc.collect()

        return valid_preds, valid_targets
            

    def fit(self,
            epochs: int = 70,
            checkpoint_dir: str = None,
            custom_name: str = f"model.pth",
            track_dir: str = f'./track/',
            threshold: float = 0.5,
            ):
        '''
            Low-effort alternative for doing the complete training and validation process
        '''
        best_precision = 0
        best_recall = 0
        best_f1 = 0
        best_auc = 0
        best_mcc = 0


        for epx in range(epochs):
            print(f"{'='*25} Epoch: {epx+1} / {epochs} {'='*25}")
            write_file(track_dir, f"{'='*25} Epoch: {epx+1} / {epochs} {'='*25}\n")

            # Result of epoch train
            train_preds, train_targets = self.train_one_epoch()
            train_loss_all = self.train_loss_fn(torch.Tensor(train_preds), torch.Tensor(train_targets))

            train_preds = [True if torch.sigmoid(torch.tensor(pred[0])) >= threshold else False for pred in train_preds]
            train_targets = [True if target[0] == 1.0 else False for target in train_targets]
            train_precision, train_recall, train_f1, train_auc, train_mcc = self.evaluation_metrics(train_preds, train_targets)
            print(f"Training Loss: {train_loss_all:.4f}")
            write_file(track_dir, f"Training Loss: {train_loss_all:.4f}\n")

            # Result of epoch valid
            valid_preds, valid_targets = self.valid_one_epoch()
            valid_loss_all = self.valid_loss_fn(torch.Tensor(valid_preds), torch.Tensor(valid_targets))

            valid_preds = [True if torch.sigmoid(torch.tensor(pred[0])) >= threshold else False for pred in valid_preds]
            valid_targets = [True if target[0] == 1.0 else False for target in valid_targets]
            valid_precision, valid_recall, valid_f1, valid_auc, valid_mcc = self.evaluation_metrics(valid_preds, valid_targets)
            print(f'Validation loss: {valid_loss_all:.4f}')
            write_file(track_dir, f"Validation loss: {valid_loss_all:.4f}\n")

            if valid_f1 > best_f1:
                best_f1 = valid_f1
                best_precision = valid_precision
                best_recall = valid_recall
                best_auc = valid_auc
                best_mcc = valid_mcc
                if checkpoint_dir is not None:
                    self.save_model(epx+1, checkpoint_dir, custom_name)
                
        print(f'Precision: {best_precision}')
        print(f'Recall   : {best_recall}')
        print(f'F1       : {best_f1}')
        print(f'AUC      : {best_auc}')
        print(f'MCC      : {best_mcc}')
        write_file(track_dir, f"{'='*65}\n")
        write_file(track_dir, f"Precision: {best_precision}\nRecall   : {best_recall}\nF1       : {best_f1}\nAUC      : {best_auc}\nMCC      : {best_mcc}\n")
        return best_precision, best_recall, best_f1, best_auc, best_mcc

    def save_model(self, epoch, path, name, verbose=False):
        '''
            Save  the model at the provided destination
        '''
        try:
            if not os.path.exists(path):
                os.makedirs(path)
        except:
            print('Errors encountered while making the output directory')
        
        state = {
            'epoch': epoch, 
            'state_dict': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
        }

        torch.save(state, os.path.join(path,f"{name}.pth"))
        if verbose:
            print(f'Model saved at: {os.path.join(path, f"{name}.pth")}')
    
    def evaluation_metrics(self, preds, targets):
        '''
            Evaluation matrics: Precision, Recall, F1, MCC
        '''
        tn, fp, fn, tp = confusion_matrix(preds, targets).ravel()
        
        # Precision
        precision = tp/(tp+fp) 
        
        # Recall
        recall = tp/(tp+fn)

        # f1-score
        fscore = metrics.f1_score(preds, targets)

        # AUC
        fpr, tpr, thresholds = metrics.roc_curve(targets, preds, pos_label=1)
        auc = metrics.auc(fpr, tpr)

        # MCC
        mcc = metrics.matthews_corrcoef(preds, targets)

        return precision, recall, fscore, auc, mcc
    
    def _convert_if_not_tensor(self, x, dtype):
        if self._tensor_check(x):
            return x.to(self.device, dtype=dtype)
        else:
            return torch.Tensor(x, dtype=dtype, device=self.device)
    
    def _tensor_check(self, x):
        return isinstance(x, torch.Tensor)